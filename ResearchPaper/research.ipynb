{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings \n",
    "from langchain_chroma import Chroma\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1\\nBoosting MCTS with Free Energy Minimization\\nMawaba Pascal Dao1, Adrian M. Peter1\\n1Florida Institute of Technology\\nKeywords: Free Energy, Monte Carlo Tree Search, Cross-Entropy Method, Infor-\\nmation Gain, Intrinsic Exploration, Continuous Action Spaces\\nAbstract\\nActive Inference, grounded in the Free Energy Principle, provides a powerful lens\\nfor understanding how agents balance exploration and goal-directed behavior in un-\\ncertain environments. Here, we propose a new planning framework, that integrates\\nMonte Carlo Tree Search (MCTS) with active inference objectives to systematically\\nreduce epistemic uncertainty while pursuing extrinsic rewards. Our key insight is that\\nMCTS—already renowned for its search efficiency—can be naturally extended to incor-\\nporate free energy minimization by blending expected rewards with information gain.\\nConcretely, the Cross-Entropy Method (CEM) is used to optimize action proposals at\\narXiv:2501.13083v1  [cs.AI]  22 Jan 2025'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = PyPDFLoader(file_path='/home/unicode/Downloads/2501.13083v1.pdf')\n",
    "document =loader.load()\n",
    "document[1].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_TOKEN = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HF_TOKEN\"] = HF_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/official/official_docs/PythonLearning/GenerativeAIWithLangchainByKrish/projects/env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name='all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_chroma.vectorstores.Chroma at 0x7fa3e3856e10>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db = Chroma.from_documents(documents=document, embedding=embeddings,persist_directory=\"./chroma_db\",collection_name=\"PDFData\",collection_metadata={\"title\":\"Research Paper\"})\n",
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever=db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'page': 6, 'source': '/home/unicode/Downloads/2501.13083v1.pdf'}, page_content='3 Background\\n3.1 Active Inference and the Free Energy Principle\\nThe Free Energy Principle, originating in neuroscience, posits that systems act to min-\\nimize a quantity called free energy, which measures how well an internal generative\\nmodel predicts observations (Friston, 2010). This principle unifies perception and ac-\\ntion under the framework of probabilistic inference, where agents aim to align their\\nbeliefs with observed data and predict future states.\\nFree energy is defined as:\\nF(Q, y) = DKL[Q(x)∥P(x|y)] − ln P(y), (1)\\nwhere:\\n• Q(x): The approximate posterior distribution over hidden states x.\\n• P(x|y): The true posterior distribution over hidden states, given observations y.\\n• ln P(y): The log evidence (marginal likelihood) of the observations.\\n• DKL[Q(x)∥P(x|y)]: The Kullback-Leibler divergence between the approximate\\nand true posterior distributions.\\nMinimizing free energy involves two components:\\n1. Reducing the Kullback-Leibler (KL) divergence, which aligns the approximate\\nposterior Q(x) with the true posterior P(x|y).\\n7')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"Free Energy\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "document[1].metadata['title'] = \"Boosting MCTS with Free Energy Minimization\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(document)):\n",
    "    document[i].metadata['title'] = \"Boosting MCTS with Free Energy Minimization\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': '/home/unicode/Downloads/2501.13083v1.pdf',\n",
       " 'page': 0,\n",
       " 'title': 'Boosting MCTS with Free Energy Minimization'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_chroma.vectorstores.Chroma at 0x7fa3e370d990>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db = Chroma.from_documents(documents=document, embedding=embeddings,persist_directory=\"./chroma_db\",collection_name=\"PDFData\",collection_metadata={\"title\":\"Research Paper\"})\n",
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': '/home/unicode/Downloads/2501.13083v1.pdf', 'page': 6, 'title': 'Boosting MCTS with Free Energy Minimization'}, page_content='3 Background\\n3.1 Active Inference and the Free Energy Principle\\nThe Free Energy Principle, originating in neuroscience, posits that systems act to min-\\nimize a quantity called free energy, which measures how well an internal generative\\nmodel predicts observations (Friston, 2010). This principle unifies perception and ac-\\ntion under the framework of probabilistic inference, where agents aim to align their\\nbeliefs with observed data and predict future states.\\nFree energy is defined as:\\nF(Q, y) = DKL[Q(x)∥P(x|y)] − ln P(y), (1)\\nwhere:\\n• Q(x): The approximate posterior distribution over hidden states x.\\n• P(x|y): The true posterior distribution over hidden states, given observations y.\\n• ln P(y): The log evidence (marginal likelihood) of the observations.\\n• DKL[Q(x)∥P(x|y)]: The Kullback-Leibler divergence between the approximate\\nand true posterior distributions.\\nMinimizing free energy involves two components:\\n1. Reducing the Kullback-Leibler (KL) divergence, which aligns the approximate\\nposterior Q(x) with the true posterior P(x|y).\\n7')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever=db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'page': 6, 'source': '/home/unicode/Downloads/2501.13083v1.pdf'}, page_content='3 Background\\n3.1 Active Inference and the Free Energy Principle\\nThe Free Energy Principle, originating in neuroscience, posits that systems act to min-\\nimize a quantity called free energy, which measures how well an internal generative\\nmodel predicts observations (Friston, 2010). This principle unifies perception and ac-\\ntion under the framework of probabilistic inference, where agents aim to align their\\nbeliefs with observed data and predict future states.\\nFree energy is defined as:\\nF(Q, y) = DKL[Q(x)∥P(x|y)] − ln P(y), (1)\\nwhere:\\n• Q(x): The approximate posterior distribution over hidden states x.\\n• P(x|y): The true posterior distribution over hidden states, given observations y.\\n• ln P(y): The log evidence (marginal likelihood) of the observations.\\n• DKL[Q(x)∥P(x|y)]: The Kullback-Leibler divergence between the approximate\\nand true posterior distributions.\\nMinimizing free energy involves two components:\\n1. Reducing the Kullback-Leibler (KL) divergence, which aligns the approximate\\nposterior Q(x) with the true posterior P(x|y).\\n7'),\n",
       " Document(metadata={'page': 6, 'source': '/home/unicode/Downloads/2501.13083v1.pdf'}, page_content='3 Background\\n3.1 Active Inference and the Free Energy Principle\\nThe Free Energy Principle, originating in neuroscience, posits that systems act to min-\\nimize a quantity called free energy, which measures how well an internal generative\\nmodel predicts observations (Friston, 2010). This principle unifies perception and ac-\\ntion under the framework of probabilistic inference, where agents aim to align their\\nbeliefs with observed data and predict future states.\\nFree energy is defined as:\\nF(Q, y) = DKL[Q(x)∥P(x|y)] − ln P(y), (1)\\nwhere:\\n• Q(x): The approximate posterior distribution over hidden states x.\\n• P(x|y): The true posterior distribution over hidden states, given observations y.\\n• ln P(y): The log evidence (marginal likelihood) of the observations.\\n• DKL[Q(x)∥P(x|y)]: The Kullback-Leibler divergence between the approximate\\nand true posterior distributions.\\nMinimizing free energy involves two components:\\n1. Reducing the Kullback-Leibler (KL) divergence, which aligns the approximate\\nposterior Q(x) with the true posterior P(x|y).\\n7'),\n",
       " Document(metadata={'page': 6, 'source': '/home/unicode/Downloads/2501.13083v1.pdf', 'title': 'Boosting MCTS with Free Energy Minimization'}, page_content='3 Background\\n3.1 Active Inference and the Free Energy Principle\\nThe Free Energy Principle, originating in neuroscience, posits that systems act to min-\\nimize a quantity called free energy, which measures how well an internal generative\\nmodel predicts observations (Friston, 2010). This principle unifies perception and ac-\\ntion under the framework of probabilistic inference, where agents aim to align their\\nbeliefs with observed data and predict future states.\\nFree energy is defined as:\\nF(Q, y) = DKL[Q(x)∥P(x|y)] − ln P(y), (1)\\nwhere:\\n• Q(x): The approximate posterior distribution over hidden states x.\\n• P(x|y): The true posterior distribution over hidden states, given observations y.\\n• ln P(y): The log evidence (marginal likelihood) of the observations.\\n• DKL[Q(x)∥P(x|y)]: The Kullback-Leibler divergence between the approximate\\nand true posterior distributions.\\nMinimizing free energy involves two components:\\n1. Reducing the Kullback-Leibler (KL) divergence, which aligns the approximate\\nposterior Q(x) with the true posterior P(x|y).\\n7'),\n",
       " Document(metadata={'page': 6, 'source': '/home/unicode/Downloads/2501.13083v1.pdf', 'title': 'Boosting MCTS with Free Energy Minimization'}, page_content='3 Background\\n3.1 Active Inference and the Free Energy Principle\\nThe Free Energy Principle, originating in neuroscience, posits that systems act to min-\\nimize a quantity called free energy, which measures how well an internal generative\\nmodel predicts observations (Friston, 2010). This principle unifies perception and ac-\\ntion under the framework of probabilistic inference, where agents aim to align their\\nbeliefs with observed data and predict future states.\\nFree energy is defined as:\\nF(Q, y) = DKL[Q(x)∥P(x|y)] − ln P(y), (1)\\nwhere:\\n• Q(x): The approximate posterior distribution over hidden states x.\\n• P(x|y): The true posterior distribution over hidden states, given observations y.\\n• ln P(y): The log evidence (marginal likelihood) of the observations.\\n• DKL[Q(x)∥P(x|y)]: The Kullback-Leibler divergence between the approximate\\nand true posterior distributions.\\nMinimizing free energy involves two components:\\n1. Reducing the Kullback-Leibler (KL) divergence, which aligns the approximate\\nposterior Q(x) with the true posterior P(x|y).\\n7')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"Free Energy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
